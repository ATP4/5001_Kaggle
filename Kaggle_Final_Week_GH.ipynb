{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the given CSV files into dataframe variables\n",
    "#Also, add csv from 1st round\n",
    "test_data = pd.read_csv(\"./input/test.csv\")\n",
    "train_data_new = pd.read_csv(\"./input/train.csv\")\n",
    "train_data_old = pd.read_csv(\"./input/train_old.csv\")\n",
    "# train_data_old.info()\n",
    "# train_data_new.info()\n",
    "\n",
    "#Concat or combine old train data with 40 rows and new train data with 400 rows\n",
    "train_data = pd.concat ([train_data_new,train_data_old], sort=False)\n",
    "#train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering Starts here\n",
    "# Replace all data in n_jobs column with -1 to 16. \n",
    "#This assumes that -1 means using all processors (according to SKLearn documentation)\n",
    "train_data.loc[train_data['n_jobs'] == -1, 'n_jobs'] = 8\n",
    "test_data.loc[test_data['n_jobs'] == -1, 'n_jobs'] = 8\n",
    "#8 is the maximum number in the train.csv provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find correlation around features provided.\n",
    "#Plot heat map using Matplotlib libraries.\n",
    "\n",
    "train_data.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "g=sns.heatmap(train_data.corr(),annot=True,cmap=\"RdYlGn\") #colormaps and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new featuer: class * cluster per class is related to each other. Divided this by n jobs.\n",
    "train_data['ncpc'] = (train_data['n_classes'] * train_data['n_clusters_per_class'])/train_data['n_jobs']\n",
    "train_data['nmis'] = (train_data['max_iter'] * train_data['n_samples'])/train_data['n_jobs']\n",
    "train_data['nfnl'] = train_data['ncpc']/train_data['n_informative']\n",
    "# print(train_data.shape)\n",
    "# train_data.head()\n",
    "# train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new featuer: class * cluster per class is related to each other. Divided this by n jobs (this applies to test data now)\n",
    "test_data['ncpc'] = (test_data['n_classes'] * test_data['n_clusters_per_class'])/test_data['n_jobs']\n",
    "#The number of classes (or labels) of the classification problem.\n",
    "#The number of clusters per class.\n",
    "test_data['nmis'] = (test_data['max_iter'] * test_data['n_samples'])/test_data['n_jobs']\n",
    "test_data['nfnl'] = test_data['ncpc']/test_data['n_informative']\n",
    "# print(test_data.shape)\n",
    "# test_data.head()\n",
    "# test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New variables being defined for time and penalty columns\n",
    "time_tag1 = train_data[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_tag_train = train_data[\"penalty\"]\n",
    "penalty_tag_test = test_data[\"penalty\"]\n",
    "train_data = train_data.drop(['time'], axis=1)\n",
    "#train_data.head(5)\n",
    "#test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes1 = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] #great variety of numerical types\n",
    "\n",
    "newdf_train = train_data.select_dtypes(include=dtypes1)\n",
    "newdf_test = test_data.select_dtypes(include=dtypes1)\n",
    "\n",
    "train_temp = (newdf_train - newdf_test.mean())/newdf_test.std(ddof=0)\n",
    "\n",
    "train = pd.concat([train_temp,penalty_tag_train],axis=1) #concat penalty column with above variable\n",
    "\n",
    "train = train.drop(columns=['l1_ratio','scale','random_state','alpha','flip_y'])\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (newdf_test - newdf_test.mean())/newdf_test.std(ddof=0)\n",
    "test = test.join(penalty_tag_test)\n",
    "test = test.drop(columns=['l1_ratio','scale','random_state','alpha','flip_y'])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow code starts here...\n",
    "#define parameters\n",
    "BATCH_SIZE = 200\n",
    "num_epochs = 10000\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, time_tag1, test_size=0.15) #split into 75/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = tf.estimator.inputs.pandas_input_fn(x=train,y=time_tag1,batch_size=BATCH_SIZE,num_epochs=num_epochs,shuffle=True)\n",
    "input_test = tf.estimator.inputs.pandas_input_fn(x=X_test,y=y_test,batch_size=BATCH_SIZE,num_epochs=num_epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = tf.feature_column.numeric_column(\"max_iter\")\n",
    "n_jobs = tf.feature_column.numeric_column(\"n_jobs\")\n",
    "n_samples = tf.feature_column.numeric_column(\"n_samples\")\n",
    "n_features = tf.feature_column.numeric_column(\"n_features\")\n",
    "n_classes = tf.feature_column.numeric_column(\"n_classes\")\n",
    "n_clusters_per_class = tf.feature_column.numeric_column(\"n_clusters_per_class\")\n",
    "n_informative = tf.feature_column.numeric_column(\"n_informative\")\n",
    "#Split each one for TF column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpc = tf.feature_column.numeric_column(\"ncpc\")\n",
    "nmis = tf.feature_column.numeric_column(\"nmis\")\n",
    "nfnl = tf.feature_column.numeric_column(\"nfnl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_tf = tf.feature_column.categorical_column_with_vocabulary_list(key=\"penalty\", vocabulary_list=[\"l2\", \"l1\", \"none\", \"elasticnet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_columns = [\n",
    "    max_iter,\n",
    "    n_jobs, \n",
    "    n_samples, \n",
    "    n_features, \n",
    "    n_classes,\n",
    "    n_clusters_per_class, \n",
    "    n_informative,\n",
    "    ncpc,\n",
    "    nmis,\n",
    "    nfnl,\n",
    "    tf.feature_column.indicator_column(penalty_tf),\n",
    "]\n",
    "\n",
    "wide_columns = [    \n",
    "    max_iter,\n",
    "    n_jobs, \n",
    "    n_samples, \n",
    "    n_features, \n",
    "    n_classes, \n",
    "    n_clusters_per_class, \n",
    "    n_informative,\n",
    "    ncpc,\n",
    "    nmis,\n",
    "    nfnl,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \" . join(Feature_columns))\n",
    "print(\", \" . join(wide_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DNNTF = tf.estimator.DNNLinearCombinedRegressor(\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=Feature_columns,\n",
    "    dnn_hidden_units=[1000, 600, 360, 150, 75, 25, 14,7],\n",
    "    dnn_activation_fn=tf.nn.leaky_relu)\n",
    "DNNTF.train(input_fn=input_train)\n",
    "\n",
    "print('Tensor Flow has finished running. Training with above feature is now ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=test,\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "predictions = m.predict(input_fn=predict_input_fn)\n",
    "result = []\n",
    "for i in predictions:\n",
    "    result.append(i[\"predictions\"][0])\n",
    "print(result)\n",
    "\n",
    "print(len(result))\n",
    "\n",
    "#loop to print results\n",
    "count=0\n",
    "for row in result:\n",
    "    if row<0:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrange test id and print final week result to be submitted to Kaggle final week competition\n",
    "test_id = np.arange(100)\n",
    "test_id = test_id.reshape(len(test_id),1)\n",
    "final_wk_result = np.array(final_wk_result)\n",
    "final_wk_result = result.reshape(len(final_wk_result),1)\n",
    "final_wk_result = np.abs(final_wk_result)\n",
    "final_wk_output = np.concatenate((test_id,result), axis=1)\n",
    "np.savetxt(\"final_week.csv\", final_wk_output, delimiter=\",\", fmt='%i,%f', header=\"Id,Time\", comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
